{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shiernee/Advanced_ML/blob/main/Week3/WOA7015_Wk3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVerYxHW-ZrQ"
   },
   "source": [
    "# The effect of imbalanced data on AUROC \n",
    "The following code evaluates the effect of imbalanced data on the AUROC of TPR-FPR curve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "c87yzg0goBrP"
   },
   "outputs": [],
   "source": [
    "# roc curve and auc on an imbalanced dataset\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcpJntDPEq2J",
    "outputId": "6e6cbb8c-8ee1-43a6-91bc-64bdfad9412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.32584935  0.21897754  0.62061895 ...  2.84071377 -0.02582733\n",
      "  -0.40885762]\n",
      " [-1.12624124 -0.86026727 -0.89264356 ... -0.92962064  0.59483549\n",
      "   1.24052468]\n",
      " [-0.48993428 -0.7453348  -1.43801838 ... -1.67525801 -0.09994425\n",
      "  -0.46569289]\n",
      " ...\n",
      " [ 0.47406074 -1.9209351   0.41681779 ...  1.04574815  1.092832\n",
      "  -0.01541749]\n",
      " [-0.62731673 -0.94336697 -1.50694171 ... -0.85092941  0.99046917\n",
      "   2.19583454]\n",
      " [ 0.88990126  0.81857103 -2.12551556 ...  1.00271323 -0.88101446\n",
      "  -0.81149645]]\n",
      "-----------\n",
      "[1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1\n",
      " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
      " 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
      " 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
      " 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
      " 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
      " 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0\n",
      " 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
      " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0\n",
      " 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
      " 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
      " 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 class dataset \n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
    "\n",
    "print(X)\n",
    "print('-----------')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLo12OKVE__J",
    "outputId": "a6dbfbe9-fecd-4d53-b281-c7193c79caa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainy - class0:  253\n",
      "trainy - class1:  247\n",
      "----------------------\n",
      "testy - class0:  249\n",
      "testy - class1:  251\n",
      "============================\n",
      "Balanced Testing date\n",
      "testy - class0:  249\n",
      "testy - class1:  249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
    "\n",
    "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
    "print('trainy - class1: ', trainy.sum())\n",
    "print('----------------------')\n",
    "print('testy - class0: ', len(testy)-testy.sum())\n",
    "print('testy - class1: ', testy.sum())\n",
    "print('============================')\n",
    "\n",
    "# make testing dataset balance\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "testX, testy = undersample.fit_resample(testX, testy)\n",
    "\n",
    "print('Balanced Testing date')\n",
    "print('testy - class0: ', len(testy)-testy.sum())\n",
    "print('testy - class1: ', testy.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3QmySaLE7Nm"
   },
   "outputs": [],
   "source": [
    "# fit a model with training data\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOu_783uGpZd"
   },
   "outputs": [],
   "source": [
    "# repeat with different skewness \n",
    "roc_list = []\n",
    "lr_acc = []\n",
    "k=1\n",
    "for i in range(0, 10):\n",
    "  pos_ind = np.where(testy==1)[0]\n",
    "  n = int(i/10 * len(pos_ind))\n",
    "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
    "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
    "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
    "  print('nth %d:positive: %d negative: %d' \n",
    "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
    "  print('---------------------------------------------')\n",
    "  \n",
    "  # predict probabilities\n",
    "  lr_probs = model.predict_proba(tmp_testX)\n",
    "  # keep probabilities for the positive outcome only\n",
    "  lr_probs = lr_probs[:, 1]\n",
    "  # calculate scores\n",
    "  lr_auc = roc_auc_score(tmp_testy, lr_probs)\n",
    "\n",
    "  # summarize scores\n",
    "  # print('iteration %d: Logistic: ROC AUC=%.3f' % (k, lr_auc))\n",
    "  k += 1\n",
    "  # calculate roc curves\n",
    "  lr_fpr, lr_tpr, _ = roc_curve(tmp_testy, lr_probs)\n",
    "  roc_list.append(lr_auc)\n",
    "\n",
    "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
    "plt.xlabel('skewness ratio')\n",
    "plt.ylabel('AUROC')\n",
    "plt.title('decreasing positive sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njFP2GHpC1VE"
   },
   "source": [
    "# Exercise 1:\n",
    "Does the AUROC (TPR vs FPR) affected by imbalanced class?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOIpcKliC56h"
   },
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4Cxp0q75PM"
   },
   "source": [
    "# The effect of imbalanced data on AUROC of PR curve and F1 score\n",
    "The following code evaluates the effect of imbalanced data on the AUROC of Precision-Recall and F1 value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oYxjJuD_8ewJ"
   },
   "outputs": [],
   "source": [
    "# roc curve and auc on an imbalanced dataset\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nr_uY_mPHLTF"
   },
   "outputs": [],
   "source": [
    "# generate 2 class dataset \n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
    "\n",
    "print(X)\n",
    "print('-----------')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbmv6pFhHWyQ"
   },
   "outputs": [],
   "source": [
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
    "\n",
    "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
    "print('trainy - class1: ', trainy.sum())\n",
    "print('----------------------')\n",
    "print('testy - class0: ', len(testy)-testy.sum())\n",
    "print('testy - class1: ', testy.sum())\n",
    "print('============================')\n",
    "\n",
    "# make testing dataset balance\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "testX, testy = undersample.fit_resample(testX, testy)\n",
    "\n",
    "print('Balanced Testing date')\n",
    "print('testy - class0: ', len(testy)-testy.sum())\n",
    "print('testy - class1: ', testy.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1ST-YEcHh5I"
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scPocodDHRp-"
   },
   "outputs": [],
   "source": [
    "# repeat with different skewness \n",
    "roc_list = []\n",
    "f1_list = []\n",
    "\n",
    "k=1\n",
    "for i in range(0, 10):\n",
    "  pos_ind = np.where(testy==1)[0]\n",
    "  n = int(i/10 * len(pos_ind))\n",
    "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
    "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
    "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
    "  print('nth %d:positive: %d negative: %d' \n",
    "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
    "  print('---------------------------------------------')\n",
    "  \n",
    "\n",
    "  # predict probabilities\n",
    "  lr_probs = model.predict_proba(tmp_testX)\n",
    "  # keep probabilities for the positive outcome only\n",
    "  lr_probs = lr_probs[:, 1]\n",
    "  # predict class values\n",
    "  yhat = model.predict(tmp_testX)\n",
    "  # calculate precision and recall for each threshold\n",
    "  lr_precision, lr_recall, _ = precision_recall_curve(tmp_testy, lr_probs)\n",
    "  # calculate scores\n",
    "  lr_f1, lr_auc = f1_score(tmp_testy, yhat), auc(lr_recall, lr_precision)\n",
    "  # summarize scores\n",
    "  # print('iteration%d Logistic: f1=%.3f auc=%.3f' % (k, lr_f1, lr_auc))\n",
    "  k += 1\n",
    "  roc_list.append(lr_auc)\n",
    "  f1_list.append(lr_f1)\n",
    "\n",
    "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
    "plt.xlabel('skewness ratio')\n",
    "plt.ylabel('AUC of PR curve')\n",
    "plt.title('decreasing positive sample')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(roc_list)), f1_list)\n",
    "plt.xlabel('skewness ratio')\n",
    "plt.ylabel('F1')\n",
    "plt.title('decreasing positive sample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DP3LOZY7v6M"
   },
   "source": [
    "# Exercise 2:\n",
    "Does the AUROC (Precision vs Recall), F1 score affected by imbalanced class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPXGu2E00-c5"
   },
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h1qUoy4DSc2"
   },
   "source": [
    "# ***Let's go back to power point - slide 13***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK3GJxwgzEjP"
   },
   "source": [
    "# Convex function\n",
    "\n",
    "This is the code to generate the graph in slide 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v0il6m3zOQb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "x = np.arange(-2, 2, 0.01)\n",
    "\n",
    "# choose one function to try\n",
    "f = lambda x: 0.5 * x ** 2 # Convex\n",
    "# f = lambda x: np.cos(np.pi * x)  # Nonconvex\n",
    "# f = lambda x: -0.5 * x ** 4  # Nonconvex\n",
    "\n",
    "filenames=[]\n",
    "for lamda in np.arange(0, 1, 0.02):\n",
    "  # LHS\n",
    "  tmp_x = lamda*x[0] + (1-lamda)*x[-1]\n",
    "\n",
    "  # RHS\n",
    "  x_line, y_line = np.array([x[0], x[-1]]), np.array([lamda*f(x[0]), (1-lamda)*f(x[-1])])\n",
    "\n",
    "  # compute LHS and RHS\n",
    "  LHS = f(tmp_x)\n",
    "  RHS = lamda*f(x[0]) + (1-lamda)*f(x[-1])\n",
    "  if LHS > RHS:\n",
    "    print('At lamda %0.3f, it is concave' % lamda)\n",
    "    print('lhs %.5f rhs %.5f' % (LHS, RHS))\n",
    "\n",
    "  plt.figure()\n",
    "  # original graph\n",
    "  plt.plot(x, f(x), label='f(x)')\n",
    "  # plot RHS\n",
    "  plt.plot(x_line, y_line, label='%0.3f' % lamda)\n",
    "  # plot LHS\n",
    "  plt.scatter(tmp_x, f(tmp_x))\n",
    "  #title, legennd\n",
    "  plt.title('lhs %.3f rhs %.3f' % (LHS, RHS))\n",
    "  plt.legend()\n",
    "  plt.savefig('lamda %0.3f.png' % lamda)\n",
    "  # plt.close()\n",
    "  filenames.append('lamda %0.3f.png' % lamda)\n",
    "\n",
    "# Build GIF\n",
    "with imageio.get_writer('mygif.gif', mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9Wuu9o0pGmg"
   },
   "source": [
    "# Understand how learning rate affects your SGD optimization\n",
    "\n",
    "We will train a neural network for a pretty simple task, i.e. calculating the exclusive-or (XOR) of two input. \n",
    "\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week3/XOR.jpg\" width=\"512\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr2IABWKoGHk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PpJZLhyndlE",
    "outputId": "b780d629-bcb3-46dd-82f4-b50699ec8d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# generate a function for XOR\n",
    "x1 = random.randint(0, 1)\n",
    "x2 = random.randint(0, 1)\n",
    "yy = 0 if (x1 == x2) else 1\n",
    "\n",
    "print('x1:', x1)\n",
    "print('x2:',x2)\n",
    "print('yy:',yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sucvJe_fntPU",
    "outputId": "3dff8ccc-9361-430c-f2a8-5fe510e90b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -1.0\n",
      "x2: 1.0\n",
      "yy: 1.0\n"
     ]
    }
   ],
   "source": [
    "x1 = random.randint(0, 1)\n",
    "x2 = random.randint(0, 1)\n",
    "yy = 0 if (x1 == x2) else 1\n",
    "\n",
    "# centered at zero\n",
    "x1 = 2. * (x1 - 0.5)\n",
    "x2 = 2. * (x2 - 0.5)\n",
    "yy = 2. * (yy - 0.5)\n",
    "\n",
    "print('x1:', x1)\n",
    "print('x2:',x2)\n",
    "print('yy:',yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEbi2kK4n9Ci",
    "outputId": "75c1ede3-5545-40bb-80bc-01fecf04192f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -0.9574273128896689\n",
      "x2: -0.9030175180760335\n",
      "yy: -0.9475814083656239\n"
     ]
    }
   ],
   "source": [
    "x1 = random.randint(0, 1)\n",
    "x2 = random.randint(0, 1)\n",
    "yy = 0 if (x1 == x2) else 1\n",
    "\n",
    "# centered at zero\n",
    "x1 = 2. * (x1 - 0.5)\n",
    "x2 = 2. * (x2 - 0.5)\n",
    "yy = 2. * (yy - 0.5)\n",
    "\n",
    "# add noise\n",
    "x1 += 0.1 * random.random()\n",
    "x2 += 0.1 * random.random()\n",
    "yy += 0.1 * random.random()\n",
    "\n",
    "print('x1:', x1)\n",
    "print('x2:',x2)\n",
    "print('yy:',yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RiENlDbpGS3"
   },
   "outputs": [],
   "source": [
    "# make it into function \n",
    "def make_data():\n",
    "    x1 = random.randint(0, 1)\n",
    "    x2 = random.randint(0, 1)\n",
    "    yy = 0 if (x1 == x2) else 1\n",
    " \n",
    "    # centered at zero\n",
    "    x1 = 2. * (x1 - 0.5)\n",
    "    x2 = 2. * (x2 - 0.5)\n",
    "    yy = 2. * (yy - 0.5)\n",
    " \n",
    "    # add noise\n",
    "    x1 += 0.1 * random.random()\n",
    "    x2 += 0.1 * random.random()\n",
    "    yy += 0.1 * random.random()\n",
    " \n",
    "    return [x1, x2, ], yy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRS81-dioL4n"
   },
   "outputs": [],
   "source": [
    "# create batch samples\n",
    "batch_size = 10\n",
    "def make_batch():\n",
    "    data = [make_data() for ii in range(batch_size)]\n",
    "    labels = [label for xx, label in data]\n",
    "    data = [xx for xx, label in data]\n",
    "    return np.array(data, dtype='float32'), np.array(labels, dtype='float32')\n",
    " \n",
    "print(make_batch())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "XB7kASWVoZJi"
   },
   "outputs": [],
   "source": [
    "# generate  500 train and 50 test data \n",
    "train_data = [make_batch() for ii in range(500)]\n",
    "test_data = [make_batch() for ii in range(50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "huxK2x7WpUGw"
   },
   "outputs": [],
   "source": [
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "BMj_0PO0ojry"
   },
   "outputs": [],
   "source": [
    "## Define our neural network class\n",
    "torch.manual_seed(42)\n",
    " \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    " \n",
    "        self.dense1 = nn.Linear(2, 2)\n",
    "        self.dense2 = nn.Linear(2, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return torch.squeeze(x)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ZqLyWCLOtrJS"
   },
   "outputs": [],
   "source": [
    "# initialize our network\n",
    "model = NN()\n",
    "\n",
    "## optimizer = stochastic gradient descent\n",
    "optimizer = optim.SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "Furjt7ZppdxA"
   },
   "outputs": [],
   "source": [
    "## train and test functions\n",
    " \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        data, target = Variable(torch.from_numpy(data)), Variable(torch.from_numpy(target))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} {}\\tLoss: {:.4f}'.format(epoch, batch_idx * len(data), loss.item()))\n",
    " \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        data, target = Variable(torch.from_numpy(data), volatile=True), Variable(torch.from_numpy(target))\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target)\n",
    "        correct += (np.around(output.data.numpy()) == np.around(target.data.numpy())).sum()\n",
    " \n",
    "    test_loss /= len(test_data)\n",
    "    test_loss = test_loss.item()\n",
    " \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, batch_size * len(test_data), 100. * correct / (batch_size * len(test_data))) )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ4c_vUzphKy",
    "outputId": "60d93a88-ce66-41e1-f823-1de85b60c26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 497 1000\tLoss: 0.4051\n",
      "Train Epoch: 497 2000\tLoss: 0.3741\n",
      "Train Epoch: 497 3000\tLoss: 0.6958\n",
      "Train Epoch: 497 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 498 0\tLoss: 0.2817\n",
      "Train Epoch: 498 1000\tLoss: 0.4051\n",
      "Train Epoch: 498 2000\tLoss: 0.3741\n",
      "Train Epoch: 498 3000\tLoss: 0.6958\n",
      "Train Epoch: 498 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 499 0\tLoss: 0.2817\n",
      "Train Epoch: 499 1000\tLoss: 0.4051\n",
      "Train Epoch: 499 2000\tLoss: 0.3741\n",
      "Train Epoch: 499 3000\tLoss: 0.6958\n",
      "Train Epoch: 499 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 500 0\tLoss: 0.2817\n",
      "Train Epoch: 500 1000\tLoss: 0.4051\n",
      "Train Epoch: 500 2000\tLoss: 0.3741\n",
      "Train Epoch: 500 3000\tLoss: 0.6958\n",
      "Train Epoch: 500 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 501 0\tLoss: 0.2817\n",
      "Train Epoch: 501 1000\tLoss: 0.4051\n",
      "Train Epoch: 501 2000\tLoss: 0.3741\n",
      "Train Epoch: 501 3000\tLoss: 0.6958\n",
      "Train Epoch: 501 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 502 0\tLoss: 0.2817\n",
      "Train Epoch: 502 1000\tLoss: 0.4051\n",
      "Train Epoch: 502 2000\tLoss: 0.3741\n",
      "Train Epoch: 502 3000\tLoss: 0.6958\n",
      "Train Epoch: 502 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 503 0\tLoss: 0.2817\n",
      "Train Epoch: 503 1000\tLoss: 0.4051\n",
      "Train Epoch: 503 2000\tLoss: 0.3741\n",
      "Train Epoch: 503 3000\tLoss: 0.6958\n",
      "Train Epoch: 503 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 504 0\tLoss: 0.2817\n",
      "Train Epoch: 504 1000\tLoss: 0.4051\n",
      "Train Epoch: 504 2000\tLoss: 0.3741\n",
      "Train Epoch: 504 3000\tLoss: 0.6958\n",
      "Train Epoch: 504 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 505 0\tLoss: 0.2817\n",
      "Train Epoch: 505 1000\tLoss: 0.4051\n",
      "Train Epoch: 505 2000\tLoss: 0.3741\n",
      "Train Epoch: 505 3000\tLoss: 0.6958\n",
      "Train Epoch: 505 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 506 0\tLoss: 0.2817\n",
      "Train Epoch: 506 1000\tLoss: 0.4051\n",
      "Train Epoch: 506 2000\tLoss: 0.3741\n",
      "Train Epoch: 506 3000\tLoss: 0.6958\n",
      "Train Epoch: 506 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 507 0\tLoss: 0.2817\n",
      "Train Epoch: 507 1000\tLoss: 0.4051\n",
      "Train Epoch: 507 2000\tLoss: 0.3741\n",
      "Train Epoch: 507 3000\tLoss: 0.6958\n",
      "Train Epoch: 507 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 508 0\tLoss: 0.2817\n",
      "Train Epoch: 508 1000\tLoss: 0.4051\n",
      "Train Epoch: 508 2000\tLoss: 0.3741\n",
      "Train Epoch: 508 3000\tLoss: 0.6958\n",
      "Train Epoch: 508 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 509 0\tLoss: 0.2817\n",
      "Train Epoch: 509 1000\tLoss: 0.4051\n",
      "Train Epoch: 509 2000\tLoss: 0.3741\n",
      "Train Epoch: 509 3000\tLoss: 0.6958\n",
      "Train Epoch: 509 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 510 0\tLoss: 0.2817\n",
      "Train Epoch: 510 1000\tLoss: 0.4051\n",
      "Train Epoch: 510 2000\tLoss: 0.3741\n",
      "Train Epoch: 510 3000\tLoss: 0.6958\n",
      "Train Epoch: 510 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 511 0\tLoss: 0.2817\n",
      "Train Epoch: 511 1000\tLoss: 0.4051\n",
      "Train Epoch: 511 2000\tLoss: 0.3741\n",
      "Train Epoch: 511 3000\tLoss: 0.6958\n",
      "Train Epoch: 511 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 512 0\tLoss: 0.2817\n",
      "Train Epoch: 512 1000\tLoss: 0.4051\n",
      "Train Epoch: 512 2000\tLoss: 0.3741\n",
      "Train Epoch: 512 3000\tLoss: 0.6958\n",
      "Train Epoch: 512 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 513 0\tLoss: 0.2817\n",
      "Train Epoch: 513 1000\tLoss: 0.4051\n",
      "Train Epoch: 513 2000\tLoss: 0.3741\n",
      "Train Epoch: 513 3000\tLoss: 0.6958\n",
      "Train Epoch: 513 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 514 0\tLoss: 0.2817\n",
      "Train Epoch: 514 1000\tLoss: 0.4051\n",
      "Train Epoch: 514 2000\tLoss: 0.3741\n",
      "Train Epoch: 514 3000\tLoss: 0.6958\n",
      "Train Epoch: 514 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 515 0\tLoss: 0.2817\n",
      "Train Epoch: 515 1000\tLoss: 0.4051\n",
      "Train Epoch: 515 2000\tLoss: 0.3741\n",
      "Train Epoch: 515 3000\tLoss: 0.6958\n",
      "Train Epoch: 515 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 516 0\tLoss: 0.2817\n",
      "Train Epoch: 516 1000\tLoss: 0.4051\n",
      "Train Epoch: 516 2000\tLoss: 0.3741\n",
      "Train Epoch: 516 3000\tLoss: 0.6958\n",
      "Train Epoch: 516 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 517 0\tLoss: 0.2817\n",
      "Train Epoch: 517 1000\tLoss: 0.4051\n",
      "Train Epoch: 517 2000\tLoss: 0.3741\n",
      "Train Epoch: 517 3000\tLoss: 0.6958\n",
      "Train Epoch: 517 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 518 0\tLoss: 0.2817\n",
      "Train Epoch: 518 1000\tLoss: 0.4051\n",
      "Train Epoch: 518 2000\tLoss: 0.3741\n",
      "Train Epoch: 518 3000\tLoss: 0.6958\n",
      "Train Epoch: 518 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 519 0\tLoss: 0.2817\n",
      "Train Epoch: 519 1000\tLoss: 0.4051\n",
      "Train Epoch: 519 2000\tLoss: 0.3741\n",
      "Train Epoch: 519 3000\tLoss: 0.6958\n",
      "Train Epoch: 519 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 520 0\tLoss: 0.2817\n",
      "Train Epoch: 520 1000\tLoss: 0.4051\n",
      "Train Epoch: 520 2000\tLoss: 0.3741\n",
      "Train Epoch: 520 3000\tLoss: 0.6958\n",
      "Train Epoch: 520 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 521 0\tLoss: 0.2817\n",
      "Train Epoch: 521 1000\tLoss: 0.4051\n",
      "Train Epoch: 521 2000\tLoss: 0.3741\n",
      "Train Epoch: 521 3000\tLoss: 0.6958\n",
      "Train Epoch: 521 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 522 0\tLoss: 0.2817\n",
      "Train Epoch: 522 1000\tLoss: 0.4051\n",
      "Train Epoch: 522 2000\tLoss: 0.3741\n",
      "Train Epoch: 522 3000\tLoss: 0.6958\n",
      "Train Epoch: 522 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 523 0\tLoss: 0.2817\n",
      "Train Epoch: 523 1000\tLoss: 0.4051\n",
      "Train Epoch: 523 2000\tLoss: 0.3741\n",
      "Train Epoch: 523 3000\tLoss: 0.6958\n",
      "Train Epoch: 523 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 524 0\tLoss: 0.2817\n",
      "Train Epoch: 524 1000\tLoss: 0.4051\n",
      "Train Epoch: 524 2000\tLoss: 0.3741\n",
      "Train Epoch: 524 3000\tLoss: 0.6958\n",
      "Train Epoch: 524 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 525 0\tLoss: 0.2817\n",
      "Train Epoch: 525 1000\tLoss: 0.4051\n",
      "Train Epoch: 525 2000\tLoss: 0.3741\n",
      "Train Epoch: 525 3000\tLoss: 0.6958\n",
      "Train Epoch: 525 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 526 0\tLoss: 0.2817\n",
      "Train Epoch: 526 1000\tLoss: 0.4051\n",
      "Train Epoch: 526 2000\tLoss: 0.3741\n",
      "Train Epoch: 526 3000\tLoss: 0.6958\n",
      "Train Epoch: 526 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 527 0\tLoss: 0.2817\n",
      "Train Epoch: 527 1000\tLoss: 0.4051\n",
      "Train Epoch: 527 2000\tLoss: 0.3741\n",
      "Train Epoch: 527 3000\tLoss: 0.6958\n",
      "Train Epoch: 527 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 528 0\tLoss: 0.2817\n",
      "Train Epoch: 528 1000\tLoss: 0.4051\n",
      "Train Epoch: 528 2000\tLoss: 0.3741\n",
      "Train Epoch: 528 3000\tLoss: 0.6958\n",
      "Train Epoch: 528 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 529 0\tLoss: 0.2817\n",
      "Train Epoch: 529 1000\tLoss: 0.4051\n",
      "Train Epoch: 529 2000\tLoss: 0.3741\n",
      "Train Epoch: 529 3000\tLoss: 0.6958\n",
      "Train Epoch: 529 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 530 0\tLoss: 0.2817\n",
      "Train Epoch: 530 1000\tLoss: 0.4051\n",
      "Train Epoch: 530 2000\tLoss: 0.3741\n",
      "Train Epoch: 530 3000\tLoss: 0.6958\n",
      "Train Epoch: 530 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 531 0\tLoss: 0.2817\n",
      "Train Epoch: 531 1000\tLoss: 0.4051\n",
      "Train Epoch: 531 2000\tLoss: 0.3741\n",
      "Train Epoch: 531 3000\tLoss: 0.6958\n",
      "Train Epoch: 531 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 532 0\tLoss: 0.2817\n",
      "Train Epoch: 532 1000\tLoss: 0.4051\n",
      "Train Epoch: 532 2000\tLoss: 0.3741\n",
      "Train Epoch: 532 3000\tLoss: 0.6958\n",
      "Train Epoch: 532 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 533 0\tLoss: 0.2817\n",
      "Train Epoch: 533 1000\tLoss: 0.4051\n",
      "Train Epoch: 533 2000\tLoss: 0.3741\n",
      "Train Epoch: 533 3000\tLoss: 0.6958\n",
      "Train Epoch: 533 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 534 0\tLoss: 0.2817\n",
      "Train Epoch: 534 1000\tLoss: 0.4051\n",
      "Train Epoch: 534 2000\tLoss: 0.3741\n",
      "Train Epoch: 534 3000\tLoss: 0.6958\n",
      "Train Epoch: 534 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 535 0\tLoss: 0.2817\n",
      "Train Epoch: 535 1000\tLoss: 0.4051\n",
      "Train Epoch: 535 2000\tLoss: 0.3741\n",
      "Train Epoch: 535 3000\tLoss: 0.6958\n",
      "Train Epoch: 535 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 536 0\tLoss: 0.2817\n",
      "Train Epoch: 536 1000\tLoss: 0.4051\n",
      "Train Epoch: 536 2000\tLoss: 0.3741\n",
      "Train Epoch: 536 3000\tLoss: 0.6958\n",
      "Train Epoch: 536 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 537 0\tLoss: 0.2817\n",
      "Train Epoch: 537 1000\tLoss: 0.4051\n",
      "Train Epoch: 537 2000\tLoss: 0.3741\n",
      "Train Epoch: 537 3000\tLoss: 0.6958\n",
      "Train Epoch: 537 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 538 0\tLoss: 0.2817\n",
      "Train Epoch: 538 1000\tLoss: 0.4051\n",
      "Train Epoch: 538 2000\tLoss: 0.3741\n",
      "Train Epoch: 538 3000\tLoss: 0.6958\n",
      "Train Epoch: 538 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 539 0\tLoss: 0.2817\n",
      "Train Epoch: 539 1000\tLoss: 0.4051\n",
      "Train Epoch: 539 2000\tLoss: 0.3741\n",
      "Train Epoch: 539 3000\tLoss: 0.6958\n",
      "Train Epoch: 539 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 540 0\tLoss: 0.2817\n",
      "Train Epoch: 540 1000\tLoss: 0.4051\n",
      "Train Epoch: 540 2000\tLoss: 0.3741\n",
      "Train Epoch: 540 3000\tLoss: 0.6958\n",
      "Train Epoch: 540 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 541 0\tLoss: 0.2817\n",
      "Train Epoch: 541 1000\tLoss: 0.4051\n",
      "Train Epoch: 541 2000\tLoss: 0.3741\n",
      "Train Epoch: 541 3000\tLoss: 0.6958\n",
      "Train Epoch: 541 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 542 0\tLoss: 0.2817\n",
      "Train Epoch: 542 1000\tLoss: 0.4051\n",
      "Train Epoch: 542 2000\tLoss: 0.3741\n",
      "Train Epoch: 542 3000\tLoss: 0.6958\n",
      "Train Epoch: 542 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 543 0\tLoss: 0.2817\n",
      "Train Epoch: 543 1000\tLoss: 0.4051\n",
      "Train Epoch: 543 2000\tLoss: 0.3741\n",
      "Train Epoch: 543 3000\tLoss: 0.6958\n",
      "Train Epoch: 543 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 544 0\tLoss: 0.2817\n",
      "Train Epoch: 544 1000\tLoss: 0.4051\n",
      "Train Epoch: 544 2000\tLoss: 0.3741\n",
      "Train Epoch: 544 3000\tLoss: 0.6958\n",
      "Train Epoch: 544 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 545 0\tLoss: 0.2817\n",
      "Train Epoch: 545 1000\tLoss: 0.4051\n",
      "Train Epoch: 545 2000\tLoss: 0.3741\n",
      "Train Epoch: 545 3000\tLoss: 0.6958\n",
      "Train Epoch: 545 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 546 0\tLoss: 0.2817\n",
      "Train Epoch: 546 1000\tLoss: 0.4051\n",
      "Train Epoch: 546 2000\tLoss: 0.3741\n",
      "Train Epoch: 546 3000\tLoss: 0.6958\n",
      "Train Epoch: 546 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 547 0\tLoss: 0.2817\n",
      "Train Epoch: 547 1000\tLoss: 0.4051\n",
      "Train Epoch: 547 2000\tLoss: 0.3741\n",
      "Train Epoch: 547 3000\tLoss: 0.6958\n",
      "Train Epoch: 547 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 548 0\tLoss: 0.2817\n",
      "Train Epoch: 548 1000\tLoss: 0.4051\n",
      "Train Epoch: 548 2000\tLoss: 0.3741\n",
      "Train Epoch: 548 3000\tLoss: 0.6958\n",
      "Train Epoch: 548 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 549 0\tLoss: 0.2817\n",
      "Train Epoch: 549 1000\tLoss: 0.4051\n",
      "Train Epoch: 549 2000\tLoss: 0.3741\n",
      "Train Epoch: 549 3000\tLoss: 0.6958\n",
      "Train Epoch: 549 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 550 0\tLoss: 0.2817\n",
      "Train Epoch: 550 1000\tLoss: 0.4051\n",
      "Train Epoch: 550 2000\tLoss: 0.3741\n",
      "Train Epoch: 550 3000\tLoss: 0.6958\n",
      "Train Epoch: 550 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 551 0\tLoss: 0.2817\n",
      "Train Epoch: 551 1000\tLoss: 0.4051\n",
      "Train Epoch: 551 2000\tLoss: 0.3741\n",
      "Train Epoch: 551 3000\tLoss: 0.6958\n",
      "Train Epoch: 551 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 552 0\tLoss: 0.2817\n",
      "Train Epoch: 552 1000\tLoss: 0.4051\n",
      "Train Epoch: 552 2000\tLoss: 0.3741\n",
      "Train Epoch: 552 3000\tLoss: 0.6958\n",
      "Train Epoch: 552 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 553 0\tLoss: 0.2817\n",
      "Train Epoch: 553 1000\tLoss: 0.4051\n",
      "Train Epoch: 553 2000\tLoss: 0.3741\n",
      "Train Epoch: 553 3000\tLoss: 0.6958\n",
      "Train Epoch: 553 4000\tLoss: 0.5083\n",
      "---------------------------------------------\n",
      "\n",
      "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
      "\n",
      "Train Epoch: 554 0\tLoss: 0.2817\n",
      "Train Epoch: 554 1000\tLoss: 0.4051\n",
      "Train Epoch: 554 2000\tLoss: 0.3741\n",
      "Train Epoch: 554 3000\tLoss: 0.6958\n"
     ]
    }
   ],
   "source": [
    "## run experiment \n",
    "nepochs = 1000\n",
    "lr = 0.001\n",
    "\n",
    "print('lr=', lr)\n",
    "for epoch in range(1, nepochs + 1):\n",
    "    train(epoch)\n",
    "    print('---------------------------------------------')\n",
    "    test()\n",
    " \n",
    " # everytime rerun this cell, please re initialize your network, and re run the train test function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM3QoS2buZe7"
   },
   "source": [
    "## Exercise 3: \n",
    "For this experiment, try the following learning rate (lr=0.0001, 0.001, 0.01, 0.1). What do you observed? <br><br>\n",
    "For example, at lr=0.001, test acc reach 100% at epoch xx... At lr=0.001, test acc reach 100% at epoch xx. As lr increases / decreases, what happen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FBd4KLZwyVB"
   },
   "source": [
    "# Acknowledgement\n",
    "\n",
    "Some of the works are inspired from \n",
    "1. Effect of learning rate on AI model = https://www.commonlounge.com/discussion/5076b2cfb2364594ba608fca3ac606bb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNIgFUm0FAZOu775EIoFBzm",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "WOA7015_Wk3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
